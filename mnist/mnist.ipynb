{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# MNIST 필기체 숫자 인식"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "코드 원 위치 : https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "'''\nA Multilayer Perceptron implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n'''\n\nfrom __future__ import print_function\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nimport tensorflow as tf\n\n# Parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of features\nn_hidden_2 = 256 # 2nd layer number of features\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder(\"float\", [None, n_input])\ny = tf.placeholder(\"float\", [None, n_classes])\n\n\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer\n\n# Store layers weight & bias\nweights = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = multilayer_perceptron(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n                                                          y: batch_y})\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n                \"{:.9f}\".format(avg_cost))\n    print(\"Optimization Finished!\")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n  return f(*args, **kwds)\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nEpoch: 0001 cost= 136.409362751\nEpoch: 0002 cost= 35.473673991\nEpoch: 0003 cost= 22.088638558\nEpoch: 0004 cost= 15.063949860\nEpoch: 0005 cost= 10.804017046\nEpoch: 0006 cost= 7.884949012\nEpoch: 0007 cost= 5.914412299\nEpoch: 0008 cost= 4.274579350\nEpoch: 0009 cost= 3.177628460\nEpoch: 0010 cost= 2.300439171\nEpoch: 0011 cost= 1.732760936\nEpoch: 0012 cost= 1.280176894\nEpoch: 0013 cost= 1.033430405\nEpoch: 0014 cost= 0.780320654\nEpoch: 0015 cost= 0.616037405\nOptimization Finished!\nAccuracy: 0.9394\n"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}